{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install\n",
    "! pip install accelerate diffusers pandas torchvision transformers wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to wandb\n",
    "! wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from PIL import Image\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DPMSolverMultistepScheduler, DiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import safetensors\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "seed = 42\n",
    "model_id = 'runwayml/stable-diffusion-v1-5'\n",
    "trigger_tokens = ['<stained-glass>', '<abin-thomas>']\n",
    "init_tokens = ['painting', 'person']\n",
    "data_dirs = ['', '']\n",
    "batch_size = 1\n",
    "num_steps = 1000\n",
    "val_freq = 100\n",
    "print_freq = 10\n",
    "checkpoint_freq = 100\n",
    "keep_n_checkpoints = 5\n",
    "validation_prompts = [\n",
    "    \"A <stained-glass> portrait of <abin-thomas>\",\n",
    "    \"<abin-thomas> sitting on a chair\",\n",
    "    \"A <stained-glass> of a puppy\",\n",
    "]\n",
    "num_val_per_prompt = 4\n",
    "output_dir = ''\n",
    "\n",
    "dataset_repeat = 100  # greater than (num_steps*batch_size)/num_images\n",
    "wandb_project = 'stained-glass-abin-thomas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init and set seed\n",
    "accelerator = Accelerator(log_with=\"wandb\")\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline components\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\")\n",
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update tokenizer\n",
    "trigger_token_ids = []\n",
    "init_token_ids = []\n",
    "for init_token, trigger_token in zip(init_tokens, trigger_tokens):\n",
    "    assert len(tokenizer.encode(init_token, add_special_tokens=False)) == 1, \"Initializer token must be a single token.\"\n",
    "\n",
    "    new_token_count = tokenizer.add_tokens([trigger_token])\n",
    "    assert new_token_count == 1, \"Placeholder token must be a new token.\"\n",
    "\n",
    "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "    token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "    trigger_token_id = tokenizer.convert_tokens_to_ids(trigger_token)\n",
    "    init_token_id = tokenizer.convert_tokens_to_ids(init_token)\n",
    "    trigger_token_ids.append(trigger_token_id)\n",
    "    init_token_ids.append(init_token_id)\n",
    "    with torch.no_grad():\n",
    "        token_embeds[trigger_token_id] = token_embeds[init_token_id].clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "class ImageCaptionDataset(Dataset):\n",
    "\n",
    "    _ADJECTIVES = [\"\", \"good\", \"cropped\", \"clean\", \"bright\", \"cool\", \"nice\", \"small\", \"large\", \"dark\", \"weird\"]\n",
    "\n",
    "    def __init__(self, data_dirs, tokenizer, trigger_tokens, repeat=100):\n",
    "        self.data_dirs = data_dirs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.repeat = repeat\n",
    "\n",
    "        filepath_caption_trigger_tuples = []\n",
    "        for data_dir, trigger_token in zip(data_dirs, trigger_tokens):\n",
    "            df = pd.read_csv(os.path.join(data_dir, 'captions.csv'), header=None)\n",
    "            filepath_caption_trigger_tuples = [(os.path.join(data_dir, 'images', row[0]), row[1].replace('\"', ''), trigger_token) for _, row in df.iterrows()]\n",
    "            filepath_caption_trigger_tuples.extend(filepath_caption_trigger_tuples)\n",
    "        np.random.shuffle(filepath_caption_trigger_tuples)\n",
    "        self.filepath_caption_trigger_map = {filepath: (caption, trigger) for filepath, caption, trigger in filepath_caption_trigger_tuples}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepath_caption_trigger_map) * self.repeat\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.filepath_caption_trigger_map)\n",
    "        example = {}\n",
    "\n",
    "        filepath = list(self.filepath_caption_trigger_map.keys())[idx]\n",
    "        caption, trigger_token = self.filepath_caption_trigger_map[filepath]\n",
    "        caption = caption.format(adjective=np.random.choice(self._ADJECTIVES), trigger=trigger_token)\n",
    "\n",
    "        image = Image.open(filepath)\n",
    "        if not image.mode == \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.resize((512, 512), resample=PIL.Image.Resampling.BICUBIC)\n",
    "        image = transforms.RandomHorizontalFlip(p=0.5)(image)\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n",
    "\n",
    "        example[\"input_ids\"] = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids[0]\n",
    "\n",
    "        return example\n",
    "    \n",
    "dataset = ImageCaptionDataset(data_dirs, tokenizer, trigger_tokens, dataset_repeat)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for training\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "text_encoder.text_model.encoder.requires_grad_(False)\n",
    "text_encoder.text_model.final_layer_norm.requires_grad_(False)\n",
    "text_encoder.text_model.embeddings.position_embedding.requires_grad_(False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(text_encoder.get_input_embeddings().parameters())\n",
    "lr_scheduler = get_scheduler(\"constant\", optimizer=optimizer)\n",
    "\n",
    "text_encoder.train()\n",
    "\n",
    "text_encoder, optimizer, dataloader, lr_scheduler = accelerator.prepare(\n",
    "    text_encoder, optimizer, dataloader, lr_scheduler\n",
    ")\n",
    "orig_embeds_params = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight.data.clone()\n",
    "\n",
    "unet.to(accelerator.device)\n",
    "vae.to(accelerator.device)\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "accelerator.init_trackers(\n",
    "    wandb_project, config={\n",
    "        'seed': seed,\n",
    "        'model_id': model_id,\n",
    "        'trigger_tokens': ', '.join(trigger_tokens),\n",
    "        'init_token': ', '.join(init_tokens),\n",
    "        'batch_size': batch_size,\n",
    "        'num_steps': num_steps,\n",
    "        'print_freq': print_freq,\n",
    "        'val_freq': val_freq,\n",
    "        'checkpoint_freq': checkpoint_freq,\n",
    "        'keep_n_checkpoints': keep_n_checkpoints,\n",
    "        'dataset_repeat': dataset_repeat,\n",
    "    })\n",
    "\n",
    "checkpoint_dir = os.path.join(output_dir, wandb_project, wandb.run.name, 'checkpoints')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "save_dir = os.path.join(output_dir, wandb_project, wandb.run.name, 'embeddings')\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe load recent checkpoint\n",
    "dirs = os.listdir(checkpoint_dir)\n",
    "dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "if path is not None:\n",
    "    accelerator.load_state(checkpoint_dir)\n",
    "    global_step = int(path.split(\"-\")[1])\n",
    "    print(f\"Loaded checkpoint from step {global_step}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "data_iter = iter(dataloader)\n",
    "while global_step < num_steps:\n",
    "    \n",
    "    try:\n",
    "        batch = next(data_iter)\n",
    "    except StopIteration:\n",
    "        data_iter = iter(dataloader)\n",
    "        batch = next(data_iter)\n",
    "\n",
    "    latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample().detach()\n",
    "    latents = latents * vae.config.scaling_factor\n",
    "    noise = torch.randn_like(latents)\n",
    "    bsz = latents.shape[0]\n",
    "    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "    timesteps = timesteps.long()\n",
    "    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "    model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "    target = noise\n",
    "    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "    accelerator.backward(loss)\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    index_no_updates = torch.ones((len(tokenizer),), dtype=torch.bool)\n",
    "    for trigger_token_id in trigger_token_ids:\n",
    "        index_no_updates[trigger_token_id] = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[\n",
    "            index_no_updates\n",
    "        ] = orig_embeds_params[index_no_updates]\n",
    "\n",
    "    global_step += 1\n",
    "\n",
    "    accelerator.log({\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}, step=global_step)\n",
    "\n",
    "    if global_step % print_freq == 0:\n",
    "        print(f\"Step: {global_step} - Loss: {loss.item()}\")\n",
    "    \n",
    "    if global_step % checkpoint_freq == 0:\n",
    "        print(f\"Saving checkpoint at step {global_step}.\")\n",
    "        checkpoints = os.listdir(checkpoint_dir)\n",
    "        checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n",
    "        checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        if len(checkpoints) >= keep_n_checkpoints:\n",
    "            num_to_remove = len(checkpoints) - keep_n_checkpoints + 1\n",
    "            removing_checkpoints = checkpoints[0:num_to_remove]\n",
    "            for removing_checkpoint in removing_checkpoints:\n",
    "                removing_checkpoint = os.path.join(checkpoint_dir, removing_checkpoint)\n",
    "                shutil.rmtree(removing_checkpoint)\n",
    "        save_path = os.path.join(checkpoint_dir, f\"checkpoint-{global_step}\")\n",
    "        accelerator.save_state(save_path)\n",
    "\n",
    "    if global_step % val_freq == 0:\n",
    "        print(f\"Evaluating at step {global_step}.\")\n",
    "        pipeline = DiffusionPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "            tokenizer=tokenizer,\n",
    "            unet=unet,\n",
    "            vae=vae,\n",
    "            safety_checker=None,\n",
    "        )\n",
    "        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "        pipeline = pipeline.to(accelerator.device)\n",
    "        pipeline.set_progress_bar_config(disable=True)\n",
    "        \n",
    "        generator = torch.Generator(device=accelerator.device).manual_seed(seed)\n",
    "        images = []\n",
    "        for prompt in validation_prompts:\n",
    "            for i in range(num_val_per_prompt):\n",
    "                image = pipeline(prompt, num_inference_steps=25, generator=generator).images[0]\n",
    "                images.append((prompt, i, image))\n",
    "        tracker = accelerator.trackers[0]\n",
    "        tracker.log({\"validation\": [wandb.Image(image, caption=f\"{i}-{prompt}\") for prompt, i, image in images]})\n",
    "        del pipeline\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save learned embeddings\n",
    "weight_name = f\"learned_embeds.safetensors\"\n",
    "save_path = os.path.join(save_dir, weight_name)\n",
    "learned_embeds_dict = {}\n",
    "for trigger_token_id, trigger_token in zip(trigger_token_ids, trigger_tokens):\n",
    "    learned_embeds = (\n",
    "        accelerator.unwrap_model(text_encoder)\n",
    "        .get_input_embeddings()\n",
    "        .weight[trigger_token_id:(trigger_token_id + 1)]\n",
    "    )\n",
    "    _learned_embeds_dict = {trigger_token: learned_embeds.detach().cpu()}\n",
    "    learned_embeds_dict.update(_learned_embeds_dict)\n",
    "safetensors.torch.save_file(learned_embeds_dict, save_path, metadata={\"format\": \"pt\"})\n",
    "\n",
    "accelerator.end_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
